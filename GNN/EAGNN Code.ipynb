{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0faf703e",
   "metadata": {},
   "source": [
    "# Install packages\n",
    "Please install all any needed packages here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812e4f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy matplotlib pandas\n",
    "%pip install torch\n",
    "%pip install torch-geometric\n",
    "%pip install torch-scatter \n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1edccc",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "\n",
    "Please import all the needed packages here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aef436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "from os import listdir\n",
    "import json\n",
    "from os.path import isfile\n",
    "from random import sample\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eadcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b65e04a",
   "metadata": {},
   "source": [
    "# Utility functions\n",
    "\n",
    "All the utility functions required for data processing, model training and post processing are defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5586cd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(x, min_val, max_val, device):\n",
    "    \n",
    "    \"\"\"Scales the data using the max and min values.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    x : an N dimensional tensor, can be node attributes or target. \n",
    "    min_val : column wise min of x, array\n",
    "    max_val : column wise max of x, array\n",
    "    device : GPU or CPU device\n",
    "    \n",
    "    Ouput\n",
    "    ------\n",
    "    a tensor of the same size as x with scaled values\n",
    "    \"\"\" \n",
    "    return (x.to(device)-min_val.to(device))/(max_val.to(device)-min_val.to(device))\n",
    "    \n",
    "def min_max_descaler(x, min_val, max_val, device):\n",
    "    \n",
    "    \"\"\"Descale the data using the max and min values.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    x : an N dimensional tensor, can be node attributes or target. \n",
    "    min_val : column wise min of x, array\n",
    "    max_val : column wise max of x, array\n",
    "    device : GPU or CPU device\n",
    "    \n",
    "    Ouput\n",
    "    ------\n",
    "    a tensor of the same size as x with descaled values\n",
    "       \n",
    "    \"\"\" \n",
    "    return (x.to(device)*(max_val.to(device)-min_val.to(device)))+ min_val.to(device)\n",
    "\n",
    "def get_max_min(data):\n",
    "    \n",
    "    \"\"\"Generating the maximum and minimum values of the training data.\n",
    "    \n",
    "    This is needed if you need to scale the data\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data : contains graph data created using Data functiom\n",
    "    \n",
    "    Ouput\n",
    "    ------\n",
    "    X_max_c, Y_max_c, E_max_c : maximum of X, Y, and  E (node attributes, target and edge attributes) for the graph, float values\n",
    "    X_min_c, Y_min_c, E_min_c : minimum of X, Y, and  E (node attributes, target and edge attributes) for the graph, float values    \n",
    "    \"\"\" \n",
    "       \n",
    "    train_loader = DataLoader(data, batch_size=20)\n",
    "    \n",
    "    X_c=[]\n",
    "    Y_c=[]\n",
    "    E_c=[]\n",
    "    \n",
    "    \n",
    "    for data in train_loader:\n",
    "        X_c.append(data.x.detach().cpu().numpy())\n",
    "        Y_c.append(data.y.detach().cpu().numpy())\n",
    "        E_c.append(data.edge_attr.detach().cpu().numpy())\n",
    "        \n",
    "        \n",
    "    # graph max and min values\n",
    "    X_c = np.vstack(X_c)\n",
    "    Y_c = np.vstack(Y_c)\n",
    "    E_c = np.vstack(E_c)\n",
    "    X_max_c = torch.Tensor(np.max(X_c,0))\n",
    "    Y_max_c = torch.Tensor(np.max(Y_c,0))\n",
    "    E_max_c = torch.Tensor(np.max(E_c,0))\n",
    "    X_min_c = torch.Tensor(np.min(X_c,0))\n",
    "    Y_min_c = torch.Tensor(np.min(Y_c,0))\n",
    "    E_min_c = torch.Tensor(np.min(E_c,0))\n",
    "    \n",
    "    \n",
    "    return X_max_c, Y_max_c, E_max_c, X_min_c, Y_min_c, E_min_c\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842ef510",
   "metadata": {},
   "source": [
    "# Graph data creation\n",
    "\n",
    "This section creates the graph data using the Data function of pytorch geometric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272299fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for farthest point sampler\n",
    "def fps(node_num,x,y,npoint): \n",
    "    N = len(node_num)\n",
    "    centroids = torch.zeros(npoint, dtype=torch.long)#.to(device)\n",
    "    distance = torch.ones(N) * 1e10 #. #.to(device)\n",
    "    farthest = torch.randint(0, N, (1,), dtype=torch.long)[0]#.to(device)\n",
    "    for i in range(npoint):\n",
    "        centroids[i] = farthest\n",
    "        centroid_x = x[farthest]\n",
    "        centroid_y = y[farthest]\n",
    "        dist = torch.from_numpy((x-centroid_x)**2 + (y-centroid_y)**2)\n",
    "        dist = dist.type(torch.float)\n",
    "        mask = dist < distance\n",
    "        distance[mask] = dist[mask]\n",
    "        farthest = torch.max(distance, -1)[1]\n",
    "    return node_num[centroids]\n",
    "\n",
    "\n",
    "## function for fake edge creation\n",
    "def fake_edge_creation(num_edges, node_list, edge_list, sample_algo='random'):\n",
    "    \n",
    "    '''This function creates fake edges.\n",
    "    num_edges is the number of fake edges needed.\n",
    "    It creates 2*num_edges edges (as they are undirected)'''\n",
    "    \n",
    "    fake_edge_list = []\n",
    "    \n",
    "    if(sample_algo=='random'):\n",
    "        # gets the list of node numbers\n",
    "        node_num = list(range(0,node_list.shape[0]))\n",
    "        while(len(fake_edge_list)<num_edges): #checking if we got the required number of fake edges\n",
    "            #sample 2 sample nodes from the node_num list\n",
    "            #and it checks if that pair is present in the existing list of edges\n",
    "            #if its not there, add this pair to the fake_edge_list\n",
    "            sampled_nodes = sample(node_num,2)\n",
    "            if(~np.isin(sampled_nodes[1],edge_list[edge_list[:,0]==sampled_nodes[0],1])):\n",
    "                fake_edge_list.append(sampled_nodes)\n",
    "    else:\n",
    "        node_num = list(range(0,node_list.shape[0]))\n",
    "        x = np.array(node_list[:,0])\n",
    "        y = np.array(node_list[:,1])\n",
    "        sampled_nodes = fps(node_num, x,y,num_edges+1)\n",
    "        for i in range(len(sampled_nodes)-1):\n",
    "            if(~np.isin(sampled_nodes[i+1],edge_list[edge_list[:,0]==sampled_nodes[i],1])):\n",
    "                fake_edge_list.append([sampled_nodes[i],sampled_nodes[i+1]])\n",
    "    return fake_edge_list\n",
    "\n",
    "\n",
    "def fake_edge_attribute_creation(edge_list,node_data):\n",
    "    \n",
    "    '''This function creates the attributes for the edges.\n",
    "    This includes x2-x1, y2-y1, distance, inverse of the deltas.'''\n",
    "    \n",
    "    edge_list = pd.DataFrame(edge_list)  #contains the list of fake edges of size Nx2\n",
    "    edge_list.columns = ['node_1', 'node_2']\n",
    "    \n",
    "    node_data = np.array(node_data).reshape(-1,2)\n",
    "    node_data = np.append(node_data, list(range(0,node_data.shape[0])), 1)\n",
    "    \n",
    "    end_node_level_data = pd.DataFrame(node_data)\n",
    "    end_node_level_data.columns = ['x','y','node'] #contains x and y coordinates and node numbers\n",
    "\n",
    "    ## Getting the nodal coordinates for the pairs of nodes of the fake edges\n",
    "    edge_list = pd.merge(edge_list, end_node_level_data[['x','y','node']], left_on='node_1', right_on='node')\n",
    "    edge_list.rename(columns = {'x':'x_1', 'y':'y_1'}, inplace = True)\n",
    "\n",
    "    edge_list = pd.merge(edge_list, end_node_level_data[['x','y','node']], left_on='node_2', right_on='node')\n",
    "    edge_list.rename(columns = {'x':'x_2', 'y':'y_2'}, inplace = True)\n",
    "\n",
    "    edge_list = edge_list.drop(columns=['node_x', 'node_y'])\n",
    "\n",
    "    ## Creating necessary edge attributes, you can change this based on your problem!!!\n",
    "    edge_list['delta_x'] = edge_list['x_2'] - edge_list['x_1']\n",
    "    edge_list['delta_y'] = edge_list['y_2'] - edge_list['y_1']\n",
    "    edge_list['inv_delta_x'] = 1./edge_list['delta_x']\n",
    "    edge_list['inv_delta_y'] = 1./edge_list['delta_y']\n",
    "\n",
    "    edge_list['distance'] = np.sqrt(edge_list['delta_x']**2 + edge_list['delta_y']**2)\n",
    "\n",
    "\n",
    "    edge_list['fake_edge'] = [1]*edge_att.shape[0]\n",
    "    \n",
    "    edge_list = edge_att.drop_duplicates().reset_index()\n",
    "    \n",
    "    return edge_list\n",
    "\n",
    "def create_fake_edge_data(fake_edge_num, node_list,edge_list):\n",
    "    \n",
    "    '''This function creates the fake edge data frame\n",
    "    with the list of fake edges and their attributes'''\n",
    "    edge_list = np.array(edge_list).reshape(-1,2)\n",
    "    fake_edge_list = fake_edge_creation(fake_edge_num, node_list, edge_list, sample_algo='random')\n",
    "    fake_edge_list=np.array(fake_edge_list).reshape(-1,2)\n",
    "    reverse_fake_edge = np.column_stack((fake_edge_list[:,1], fake_edge_list[:,0]))\n",
    "    fake_edge_list = np.concatenate((fake_edge_list, reverse_fake_edge), axis=0)\n",
    "    fake_edge_data_df = pd.DataFrame(fake_edge_list)\n",
    "    fake_edge_att = fake_edge_attribute_creation(fake_edge_data_df,node_list)\n",
    "    \n",
    "    return fake_edge_list, fake_edge_att\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a4955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_graph_data(source_dir, sim, fake_perc):\n",
    "    \n",
    "    \"\"\"Loading the data from the source directory and returns the data\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    source_dir : source directory which contains the original graph data, with the name format 'run_{sim}'.\n",
    "    sim : simulation/run number, to identify different simulations, int\n",
    "    fake_perc : % of augmented edges\n",
    "    Ouput\n",
    "    ------\n",
    "    data - Pytorch geometric Data object\n",
    "       \n",
    "    \"\"\" \n",
    "    \n",
    "    data = torch.load(osp.join('{}/run_{}.pt'.format(source_dir,sim)))\n",
    "    \n",
    "    node_data = data.x[:,:2]\n",
    "    edge_data = data.edge_index.t().reshape(-1,2)\n",
    "\n",
    "    fake_edge_num = round(edge_data.shape[0]*fake_perc)\n",
    "    fake_edge_list, fake_edge_att = create_fake_edge_data(fake_edge_num, node_data,edge_data)\n",
    "    fake_edge_att = np.array(fake_edge_att)\n",
    "    \n",
    "    edge_attr = np.array(data.edge_attr)\n",
    "    \n",
    "    #adding fake_flag=0 to the edge_attributes of real edges\n",
    "    edge_attr = np.append(edge_attr, np.zeros(edge_attr.shape[0]),1)\n",
    "    \n",
    "    # adding fake edges and the attributes to the real edge data\n",
    "    edge_attr = np.concatenate([edge_attr,fake_edge_att],axis=0)\n",
    "    edge_connection = np.concatenate([np.array(edge_data),fake_edge_list],axis=0)\n",
    "    \n",
    "    edge_attr = torch.Tensor(edge_attr)\n",
    "    edge_connection = torch.Tensor(edge_connection, dtype=torch.long)\n",
    "    \n",
    "    data.edge_index = edge_connection.t().contiguous()\n",
    "    data.edge_attr = edge_attr\n",
    "    \n",
    "    #DO THE REST OF THE PROCESSING BASED ON YOUR DATA!!\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db23c119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    Class to create custom graph dataset compatible for EAGNN\n",
    "    \n",
    "    Every graph data generated using this class contains graph of Data object.\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    def __init__(self, root, source_dir, sim_list, fake_perc, test=False, transform=None, pre_transform=None):\n",
    "        \n",
    "        self.root = root # root directory where procssed data is stored\n",
    "        self.sims = sim_list # list of simulation numbers (different for train and test data)\n",
    "        self.test = test # flag to identify test data\n",
    "        self.source_dir = source_dir # source directory for raw data\n",
    "        self.fake_perc = fake_perc\n",
    "        \n",
    "        super(GraphDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return []\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def len(self):\n",
    "        return len(self.sims)\n",
    "    \n",
    "    def process(self):\n",
    "        i = 0\n",
    "        \n",
    "        for i in range(len(self.sims)):\n",
    "            print(\"processing simulation {}!\".format(i))\n",
    "            data = process_graph_data(self.source_dir, self.sims[i],self.fake_perc)\n",
    "            \n",
    "            if(self.test):\n",
    "                torch.save(data, osp.join(self.root, 'processed/test_run_{}.pt'.format(i)))\n",
    "                \n",
    "            else:\n",
    "                torch.save(data, osp.join(self.root, 'processed/run_{}.pt'.format(i)))\n",
    "\n",
    "    def get(self,idx):\n",
    "        \n",
    "        if(self.test):\n",
    "            i = idx \n",
    "            data = torch.load(osp.join(self.root, 'processed/test_run_{}.pt'.format(i)))\n",
    "            \n",
    "        else:\n",
    "            i = idx \n",
    "            data = torch.load(osp.join(self.root, 'processed/run_{}.pt'.format(i)))\n",
    "            \n",
    "        return data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb5a9eb",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b70e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the list of simulation numbers from the dataset\n",
    "### change this based on your data!!!\n",
    "mypath = 'dataset/'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "file_nums=[]\n",
    "for file in onlyfiles:\n",
    "    file_nums.append(int(file.split('_')[1]))\n",
    "file_nums = np.unique(np.array(file_nums)) \n",
    "\n",
    "# defining the root and source directory\n",
    "# root directory/processed is the folder where the processed graph data is stored\n",
    "# source directory contains the raw graph data\n",
    "root_dir = \"dataset/\"\n",
    "source_dir = \"dataset\"\n",
    "\n",
    "# 500 simulations are used for training and 300 for testing\n",
    "# you can changes these numbers\n",
    "train_list = file_nums[:500]\n",
    "val_list = file_nums[500:]\n",
    "\n",
    "#change batch size and percentage of fake edges, if needed\n",
    "batch_size = 2\n",
    "fake_perc = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61f152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing training and validation datasets\n",
    "train_dataset = GraphDataset(root_dir, source_dir, train_list, fake_perc=fake_perc,test=False)\n",
    "val_dataset = GraphDataset(root_dir, source_dir, val_list, fake_perc=fake_perc,test=True)\n",
    "\n",
    "## Defining the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "## Calculating the min, max values of node, edge attributes and target for scaling purposes\n",
    "xmax,ymax,emax,xmin,ymin,emin = get_max_min(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f7e7f9",
   "metadata": {},
   "source": [
    "# GNN Model Architecture\n",
    "\n",
    "Next section contains the functions and classes which define the Multi-fidelity Graph U-Net architecture\n",
    "\n",
    "Classes EdgeModel and NodeModel contain the edge and node update MLP functions for message passing and propagation\n",
    "\n",
    "Net_EAGNN class contains the network architecture for a GNN model. It has the similar architecture as that of Meshgraphnet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b3a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeModel (torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Class for updating the edge attributes of the graph. \n",
    "    \n",
    "    It is a MLP network with a single hidden layer, with ReLU activation function\n",
    "    Input consists of edge attributes node attributes of the edges and the node attributes of the two nodes connected by the edge\n",
    "    \n",
    "    Output is the updated edge attribites of all the edges of the graph\n",
    "    \n",
    "    If residuals is True, the updated values are added to the previous edge attributes before they are returned.\n",
    "    This is similar to the residual network.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, n_edge_features, hiddens, n_targets, residuals):\n",
    "        super().__init__()\n",
    "        self.residuals = residuals\n",
    "        self. edge_mlp = Seq(\n",
    "            Linear(2*n_features + n_edge_features, hiddens),\n",
    "            ReLU(),\n",
    "            Linear (hiddens, n_targets),\n",
    "        )\n",
    "        \n",
    "    def forward(self, src, dest, edge_attr, u=None, batch=None):\n",
    "        #Concats the nodes connecting the edges and edge attributes and passed through MLP\n",
    "        #src and dest are the node attributes of the two nodes\n",
    "        #edge_attr is the edge attributes of the edge connecting the two nodes\n",
    "        \n",
    "        out = torch.cat([src, dest, edge_attr], 1)\n",
    "        out = self.edge_mlp(out)\n",
    "        if self.residuals:\n",
    "            out = out + edge_attr\n",
    "        return out\n",
    "\n",
    "class NodeModel (torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Class for updating the node attributes of the graph. \n",
    "    \n",
    "    It consists of two MLP networks. Both the networks have single hidden layer with with ReLU activation function.\n",
    "    \n",
    "    First MLP network, node_mlp_1, takes the node attributes of the neighboring nodes and the updated edge attributes of the\n",
    "    edges connecting these nodes as the input. Output is the message from the neighboring nodes, with the same size as the\n",
    "    node attributes. \n",
    "    \n",
    "    Second MLP network, node_mlp_2, takes the aggregated message acorss all the neighboring nodes from node_mlp_1 \n",
    "    and the node attributes of the current node as the input. Output is the updated node attributes. \n",
    "    \n",
    "    If residuals is True, the updated values are added to the previous node attributes before they are returned.\n",
    "    This is similar to the residual network.\n",
    "    \"\"\" \n",
    "    \n",
    "    def __init__(self, n_features, n_edge_features, hiddens, n_targets, residuals):\n",
    "        super(NodeModel, self).__init__()\n",
    "        \n",
    "        self.residuals = residuals\n",
    "        \n",
    "        #message calculation MLP\n",
    "        self. node_mlp_1 = Seq(\n",
    "            Linear(n_features + n_edge_features, hiddens),\n",
    "            ReLU(),\n",
    "            Linear(hiddens, n_targets),\n",
    "        )\n",
    "        \n",
    "        #node attribute update MLP\n",
    "        self.node_mlp_2 = Seq(\n",
    "            Linear (hiddens + n_features, hiddens),\n",
    "            ReLU(),\n",
    "            Linear(hiddens, n_targets),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        row, col = edge_index\n",
    "        out = torch. cat([x[col], edge_attr], dim=1)\n",
    "        out = self.node_mlp_1(out) #message calculation\n",
    "        out = torch_scatter.scatter_add(out, row, dim=0, dim_size=x.size(0)) #message aggregation, aggregation function is SUM\n",
    "        out = torch.cat([x, out], dim=1)\n",
    "        out = self.node_mlp_2(out) #node update\n",
    "        if self.residuals:\n",
    "            out = out + x\n",
    "        return out\n",
    "\n",
    "\n",
    "def build_layer(n_features, n_edge_features, hiddens, n_targets, batchnorm=False, residuals=True):\n",
    "    \"\"\"Calling the edge and node update modules\n",
    "    \"\"\" \n",
    "    return geom_nn.MetaLayer(\n",
    "        edge_model=EdgeModel(n_features, n_edge_features, hiddens, n_targets, residuals=residuals),\n",
    "        node_model=NodeModel(n_features, n_edge_features, hiddens, n_targets, residuals=residuals),\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d33dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_EAGNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Class defining the GNN architecture\n",
    "    \n",
    "    \"\"\" \n",
    "    def __init__(self, node_att_num, edge_att_num, output_dim, \n",
    "                 gn_depth, encoder_neurons, node_index_list, \n",
    "                 dropout=0.1, batch_norm = False):\n",
    "        super(Net_EAGNN, self).__init__()\n",
    "\n",
    "        self.node_att_num = node_att_num #number of node attributes\n",
    "        self.edge_att_num = edge_att_num #number of edge attributes\n",
    "        self.encoder_neurons = encoder_neurons #number of neurons in each layer of encoder network\n",
    "        self.output_dim = output_dim #number of outputs/targets\n",
    "        self.gn_depth = gn_depth #list containing the number of GN blocks for each level\n",
    "        self.node_index_list = node_index_list #indices of the node features to be used for training\n",
    "        self.dropout = dropout #droput ratio\n",
    "        self.channel = self.encoder_neurons[1] #latent space size after encoding\n",
    "        # defining the encoder network for node attributes\n",
    "        self.encoder_node = Seq(Linear(self.node_att_num, self.encoder_neurons[0]),\n",
    "                                ReLU(),\n",
    "                                Linear(self.encoder_neurons[0], self.encoder_neurons[1]))\n",
    "        # defining the encoder network for edge attributes\n",
    "        self.encoder_edge = Seq(Linear(self.edge_att_num, self.encoder_neurons[0]),\n",
    "                                ReLU(),\n",
    "                                Linear(self.encoder_neurons[0], self.encoder_neurons[1]))\n",
    "        # defining the decoder network \n",
    "        self.decoder = Seq(Linear(self.encoder_neurons[1], self.encoder_neurons[0]),\n",
    "                                ReLU(),\n",
    "                                Linear(self.encoder_neurons[0], self.output_dim))\n",
    "        # defining batch norm if used\n",
    "        self.bn = torch.nn.BatchNorm1d(self.channel)\n",
    "        \n",
    "        # defining the GN blocks for message passing and aggregation\n",
    "        self.GN_Blocks= torch.nn.ModuleList()\n",
    "        for i in range(self.gn_depth):\n",
    "            self.GN_Blocks.append(build_layer(self.channel,self.channel,self.channel,self.channel))\n",
    "        \n",
    "    def forward(self, data, device, scale=True):\n",
    "                \n",
    "        data = data.to(device)\n",
    "\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        \n",
    "        if scale:\n",
    "            # scaling node attributes\n",
    "            x = min_max_scaler(x, xmin, xmax, device)\n",
    "            # scaling edge attributes\n",
    "            edge_attr = min_max_scaler(edge_attr, emin, emax, device)\n",
    "\n",
    "        # encoding the node attributes for the graphs\n",
    "        x = self.encoder_node(x[:,self.node_index_list])\n",
    "        \n",
    "        # encoding the edge attributes for the graphs\n",
    "        edge_attr = self.encoder_edge(torch.tensor(edge_attr, dtype=torch.float))\n",
    "        \n",
    "        #  node attributes of coarse graph are  passed through the entire set of GN blocks\n",
    "        for i in range(self.gn_depth_list):\n",
    "            x, edge_attr, _ = self.GN_Blocks[i](x, edge_index, edge_attr=edge_attr)\n",
    "            \n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4b825a",
   "metadata": {},
   "source": [
    "# Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99c690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(data_loader, loss_all, device, scale):\n",
    "    \"\"\"Training the GNN model\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data_loader : Data loader object from pytorch geometric, it contains all the graphs for training\n",
    "    loss_all : loss value, Tensor float\n",
    "    device : GPU/CPU\n",
    "    scale : if True, scaling is done on node and edge attributes as well as the target, boolean\n",
    "    \n",
    "    Ouput\n",
    "    ------\n",
    "    loss_all : loss value after a single epoch, Tensor float\n",
    "       \n",
    "    \"\"\" \n",
    "    model.train()\n",
    "    for data in data_loader:\n",
    "        # get the predicted outputs\n",
    "        out = model(data, device, scale)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # scale target responses if scale is True\n",
    "        if scale:\n",
    "            y =  min_max_scaler(data.y, ymin, ymax, device).reshape(-1,1) #change the reshape based on the number of outputs\n",
    "        else:\n",
    "            y = data[0].y.reshape(-1,1)\n",
    "            \n",
    "        # loss calculation\n",
    "        loss_calc = loss(out.reshape(-1,1), y.reshape(-1,1)) \n",
    "        loss_calc.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        my_lr_scheduler.step()\n",
    "        \n",
    "    return loss_all\n",
    "\n",
    "\n",
    "def model_eval(data_loader, device, scale):\n",
    "    \"\"\"Evaluating the GNN model\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data_loader : Data loader object from pytorch geometric, it contains all the graphs for training\n",
    "    device : GPU/CPU\n",
    "    scale : if True, scaling is done on node and edge attributes as well as the target, boolean\n",
    "    \n",
    "    Ouput\n",
    "    ------\n",
    "    l2_err : relative L2 error for the predictions in the graph, float\n",
    "       \n",
    "    \"\"\" \n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    for data in data_loader:\n",
    "                \n",
    "        #getting the prediction from just the fine graph\n",
    "        pred = model(data, device, scale)\n",
    "        \n",
    "        if scale:\n",
    "            pred = min_max_descaler(pred, ymin, ymax, device).detach().cpu().numpy().reshape(-1,1)\n",
    "        else:\n",
    "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
    "            \n",
    "        label = data.y.detach().cpu().numpy().reshape(-1,1)\n",
    "        predictions.append(pred)\n",
    "        labels.append(label)\n",
    "        \n",
    "    predictions = np.vstack(predictions)\n",
    "    labels = np.vstack(labels)\n",
    "    \n",
    "    # calculation of relative L2 error\n",
    "    diff_norm = np.linalg.norm(predictions - labels, ord=2)\n",
    "    y_norm = np.linalg.norm(labels, ord=2)\n",
    "    l2_err = np.mean(diff_norm / y_norm)\n",
    "\n",
    "    return l2_err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dec07f",
   "metadata": {},
   "source": [
    "### CHANGE TRAINING AND MODEL PARAMETERS HERE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f79605",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TRAINING HYPERPARAMETERS\n",
    "n_epochs = 1000\n",
    "batch_size = 2\n",
    "lr = 0.001\n",
    "weight_decay=1e-6\n",
    "\n",
    "# MODEL PARAMETERS\n",
    "# change these based on your data\n",
    "node_att_num = 10\n",
    "edge_att_num = 5\n",
    "output_dim = 1\n",
    "gn_depth = 12\n",
    "encoder_neurons = [64, 128]\n",
    "node_index_list = [0,1,4,5,6,7,8,9,10,11]\n",
    "dropout = 0.0\n",
    "scale = True\n",
    "batch_norm = False\n",
    "\n",
    "# DIRECTORIES TO STORE RESULTS\n",
    "result_dir = 'results'\n",
    "model_dir = 'models'\n",
    "loss_dir = 'losses'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a0c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Net_EAGNN(node_att_num=node_att_num, edge_att_num=edge_att_num, output_dim=output_dim,\n",
    "                gn_depth=gn_depth, node_index_list=node_index_list,encoder_neurons=encoder_neurons, \n",
    "                dropout=dropout, batch_norm = False).to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.99, 0.999), weight_decay=weight_decay)\n",
    "my_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=n_epochs, eta_min=1e-8)\n",
    "\n",
    "\n",
    "loss = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8c092",
   "metadata": {},
   "source": [
    "### MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model training\n",
    "epoch_list = []\n",
    "train_l2_err = []\n",
    "val_l2_err = []\n",
    "\n",
    "print('Training started...')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss_all = 0\n",
    "    loss_all = model_train(train_loader, loss_all, device, scale)\n",
    "    if(epoch%10==0):\n",
    "        epoch_list.append(epoch)\n",
    "        l2_err = model_eval(train_loader, device, scale)\n",
    "        train_l2_err.append(l2_err)\n",
    "        l2_err = model_eval(val_loader, device, scale)\n",
    "        val_l2_err.append(l2_err)\n",
    "        print('epoch: ', epoch, 'train error: ', train_l2_err[-1], 'val error: ', val_l2_err[-1])\n",
    "        print()\n",
    "\n",
    "        # saving the model\n",
    "        torch.save({\n",
    "            'epoch':epoch,\n",
    "            'model_state_dict':model.state_dict(),\n",
    "            'optimizer_state_dict':optimizer.state_dict(),\n",
    "        }, result_dir + '/' + model_dir + '/model_eagnn.pt')\n",
    "        \n",
    "        # saving the loss results\n",
    "        np.savetxt(result_dir + '/' + loss_dir + '/train_l2_err.txt', train_l2_err)\n",
    "        np.savetxt(result_dir + '/' + loss_dir + '/val_l2_err.txt', val_l2_err)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
