{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0faf703e",
   "metadata": {},
   "source": [
    "# Install packages\n",
    "Please install all any needed packages here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812e4f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy matplotlib pandas\n",
    "%pip install torch\n",
    "%pip install torch-geometric\n",
    "%pip install torch-scatter \n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1edccc",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "\n",
    "Please import all the needed packages here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aef436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.nn as geom_nn\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch_scatter\n",
    "import os.path as osp\n",
    "from random import sample\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_geometric\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eadcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b65e04a",
   "metadata": {},
   "source": [
    "# Utility functions\n",
    "\n",
    "All the utility functions required for data processing, model training and post processing are defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5586cd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(x, min_val, max_val, device):\n",
    "    \n",
    "    \"\"\"Scales the data using the max and min values.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    x : an N dimensional tensor, can be node attributes or target. \n",
    "    min_val : column wise min of x, array\n",
    "    max_val : column wise max of x, array\n",
    "    device : GPU or CPU device\n",
    "    \n",
    "    Ouput\n",
    "    ------\n",
    "    a tensor of the same size as x with scaled values\n",
    "    \"\"\" \n",
    "    return (x.to(device)-min_val.to(device))/(max_val.to(device)-min_val.to(device))\n",
    "    \n",
    "def min_max_descaler(x, min_val, max_val, device):\n",
    "    \n",
    "    \"\"\"Descale the data using the max and min values.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    x : an N dimensional tensor, can be node attributes or target. \n",
    "    min_val : column wise min of x, array\n",
    "    max_val : column wise max of x, array\n",
    "    device : GPU or CPU device\n",
    "    \n",
    "    Ouput\n",
    "    ------\n",
    "    a tensor of the same size as x with descaled values\n",
    "       \n",
    "    \"\"\" \n",
    "    return (x.to(device)*(max_val.to(device)-min_val.to(device)))+ min_val.to(device)\n",
    "\n",
    "def get_max_min(data):\n",
    "    \n",
    "    \"\"\"Generating the maximum and minimum values of the training data.\n",
    "    \n",
    "    This is needed if you need to scale the data\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data : contains a list of graph data\n",
    "        Each of them are grapha data created using Data function \n",
    "        and contains three graphs of different resolutions - coarse, medium and fine\n",
    "    \n",
    "    Ouput\n",
    "    ------\n",
    "    X_max_c, Y_max_c, E_max_c : maximum of X, Y, and  E (node attributes, target and edge attributes) for the coarse graph, float values\n",
    "    X_min_c, Y_min_c, E_min_c : minimum of X, Y, and  E (node attributes, target and edge attributes) for the coarse graph, float values\n",
    "    X_max_m, Y_max_m, E_max_m : maximum of X, Y, and  E (node attributes, target and edge attributes) for the medium graph, float values\n",
    "    X_min_m, Y_min_m, E_min_m : minimum of X, Y, and  E (node attributes, target and edge attributes) for the medium graph, float values\n",
    "    X_max_f, Y_max_f, E_max_f : maximum of X, Y, and  E (node attributes, target and edge attributes) for the fine graph, float values\n",
    "    X_min_f, Y_min_f, E_min_f : minimum of X, Y, and  E (node attributes, target and edge attributes) for the fine graph, float values\n",
    "    \n",
    "    \"\"\" \n",
    "       \n",
    "    train_loader = DataLoader(data, batch_size=20)\n",
    "    \n",
    "    X_c=[]\n",
    "    Y_c=[]\n",
    "    E_c=[]\n",
    "    \n",
    "    X_m=[]\n",
    "    Y_m=[]\n",
    "    E_m=[]\n",
    "    \n",
    "    X_f=[]\n",
    "    Y_f=[]\n",
    "    E_f=[]\n",
    "    \n",
    "    for data in train_loader:\n",
    "        X_c.append(data[0].x.detach().cpu().numpy())\n",
    "        Y_c.append(data[0].y.detach().cpu().numpy())\n",
    "        E_c.append(data[0].edge_attr.detach().cpu().numpy())\n",
    "        \n",
    "        X_m.append(data[1].x.detach().cpu().numpy())\n",
    "        Y_m.append(data[1].y.detach().cpu().numpy())\n",
    "        E_m.append(data[1].edge_attr.detach().cpu().numpy())\n",
    "        \n",
    "        X_f.append(data[2].x.detach().cpu().numpy())\n",
    "        Y_f.append(data[2].y.detach().cpu().numpy())\n",
    "        E_f.append(data[2].edge_attr.detach().cpu().numpy())\n",
    "        \n",
    "    # coarse graph max and min values\n",
    "    X_c = np.vstack(X_c)\n",
    "    Y_c = np.vstack(Y_c)\n",
    "    E_c = np.vstack(E_c)\n",
    "    X_max_c = torch.Tensor(np.max(X_c,0))\n",
    "    Y_max_c = torch.Tensor(np.max(Y_c,0))\n",
    "    E_max_c = torch.Tensor(np.max(E_c,0))\n",
    "    X_min_c = torch.Tensor(np.min(X_c,0))\n",
    "    Y_min_c = torch.Tensor(np.min(Y_c,0))\n",
    "    E_min_c = torch.Tensor(np.min(E_c,0))\n",
    "    \n",
    "    # medium graph max and min values\n",
    "    X_m = np.vstack(X_m)\n",
    "    Y_m = np.vstack(Y_m)\n",
    "    E_m = np.vstack(E_m)\n",
    "    X_max_m = torch.Tensor(np.max(X_m,0))\n",
    "    Y_max_m = torch.Tensor(np.max(Y_m,0))\n",
    "    E_max_m = torch.Tensor(np.max(E_m,0))\n",
    "    X_min_m = torch.Tensor(np.min(X_m,0))\n",
    "    Y_min_m = torch.Tensor(np.min(Y_m,0))\n",
    "    E_min_m = torch.Tensor(np.min(E_m,0))\n",
    "    \n",
    "    # fine graph max and min values\n",
    "    X_f = np.vstack(X_f)\n",
    "    Y_f = np.vstack(Y_f)\n",
    "    E_f = np.vstack(E_f)\n",
    "    X_max_f = torch.Tensor(np.max(X_f,0))\n",
    "    Y_max_f = torch.Tensor(np.max(Y_f,0))\n",
    "    E_max_f = torch.Tensor(np.max(E_f,0))\n",
    "    X_min_f = torch.Tensor(np.min(X_f,0))\n",
    "    Y_min_f = torch.Tensor(np.min(Y_f,0))\n",
    "    E_min_f = torch.Tensor(np.min(E_f,0))\n",
    "    \n",
    "    return X_max_c, Y_max_c, E_max_c, X_min_c, Y_min_c, E_min_c, X_max_m, Y_max_m, E_max_m, X_min_m, Y_min_m, E_min_m, X_max_f, Y_max_f, E_max_f, X_min_f, Y_min_f, E_min_f\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842ef510",
   "metadata": {},
   "source": [
    "# Graph data creation\n",
    "\n",
    "This section creates the graph data using the Data function of pytorch geometric.\n",
    "\n",
    "Each graph data object contains 3 graphs, of the same simulation but with different resolutions.\n",
    "\n",
    "We identify them as - coarse, medium and fine (differentiated with _c, _m and _f in the variable names).\n",
    "\n",
    "Coarse graph is the lowest resolution followed by medium and fine graphs respectively. Fine graph has the highest resolution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a4955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_graph_data(source_dir, sim):\n",
    "    \n",
    "    \"\"\"Loading the data from the source directory and returns the data\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    source_dir : source directory which contains the original graph data, with the name format 'run_{sim}_{resolution}'.\n",
    "                resolution can be LF/MF/HF, referring to coarse (low), medium and high resolution data respectively\n",
    "    sim : simulation/run number, to identify different simulations, int\n",
    "    \n",
    "    Ouput\n",
    "    ------\n",
    "    data_c - coarse resolution data, Pytorch geometric Data object\n",
    "    data_m - medium resolution data, Pytorch geometric Data object\n",
    "    data_f - fine resolution data, Pytorch geometric Data object\n",
    "       \n",
    "    \"\"\" \n",
    "    \n",
    "    data_c = torch.load(osp.join('{}/run_{}_LF.pt'.format(source_dir,sim)))\n",
    "    data_m = torch.load(osp.join('{}/run_{}_MF.pt'.format(source_dir,sim)))\n",
    "    data_f = torch.load(osp.join('{}/run_{}_HF.pt'.format(source_dir,sim)))\n",
    "    \n",
    "    #DO THE REST OF THE PROCESSING BASED ON YOUR DATA!!\n",
    "    \n",
    "#     # The following edge index correction is done as, in the original data, edge index numbering starts from 1. \n",
    "#     # It should start from 0 for correct pytorch geometric data processing\n",
    "#     data_c.edge_index[0,:] = (data_c.edge_index[0,:]-1).long()\n",
    "#     data_c.edge_index[1,:] = (data_c.edge_index[1,:]-1).long()\n",
    "#     data_m.edge_index[0,:] = (data_m.edge_index[0,:]-1).long()\n",
    "#     data_m.edge_index[1,:] = (data_m.edge_index[1,:]-1).long()\n",
    "#     data_f.edge_index[0,:] = (data_f.edge_index[0,:]-1).long()\n",
    "#     data_f.edge_index[1,:] = (data_f.edge_index[1,:]-1).long()\n",
    "    \n",
    "    return data_c, data_m, data_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db23c119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFGraphDataset(Dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    Class to create custom multi-fidelity graph dataset compatible for Multi-fidelity Graph U-Net architecture\n",
    "    \n",
    "    Every graph data generated using this class contains three graphs of Data object, with three different resolutions.\n",
    "    \n",
    "    The output of the class is a list of length 3 containing these three graphs.\n",
    "    \"\"\" \n",
    "    \n",
    "    def __init__(self, root, source_dir, sim_list, test=False, transform=None, pre_transform=None):\n",
    "        \n",
    "        self.root = root # root directory where procssed data is stored\n",
    "        self.sims = sim_list # list of simulation numbers (different for train and test data)\n",
    "        self.test = test # flag to identify test data\n",
    "        self.source_dir = source_dir # source directory for raw data\n",
    "        \n",
    "        super(MFGraphDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return []\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def len(self):\n",
    "        return len(self.sims)\n",
    "    \n",
    "    def process(self):\n",
    "        i = 0\n",
    "        \n",
    "        for i in range(len(self.sims)):\n",
    "            print(\"processing simulation {}!\".format(i))\n",
    "            data_c, data_m, data_f = process_graph_data(self.source_dir, self.sims[i])\n",
    "            \n",
    "            if(self.test):\n",
    "                torch.save(data_c, osp.join(self.root, 'processed/test_run_{}_LF.pt'.format(i)))\n",
    "                torch.save(data_m, osp.join(self.root, 'processed/test_run_{}_MF.pt'.format(i)))\n",
    "                torch.save(data_f, osp.join(self.root, 'processed/test_run_{}_HF.pt'.format(i)))\n",
    "                \n",
    "            else:\n",
    "                torch.save(data_c, osp.join(self.root, 'processed/run_{}_LF.pt'.format(i)))\n",
    "                torch.save(data_m, osp.join(self.root, 'processed/run_{}_MF.pt'.format(i)))\n",
    "                torch.save(data_f, osp.join(self.root, 'processed/run_{}_HF.pt'.format(i)))\n",
    "\n",
    "    def get(self,idx):\n",
    "        \n",
    "        if(self.test):\n",
    "            i = idx \n",
    "            data_c = torch.load(osp.join(self.root, 'processed/test_run_{}_LF.pt'.format(i)))\n",
    "            data_m = torch.load(osp.join(self.root, 'processed/test_run_{}_MF.pt'.format(i)))\n",
    "            data_f = torch.load(osp.join(self.root, 'processed/test_run_{}_HF.pt'.format(i)))\n",
    "            \n",
    "        else:\n",
    "            i = idx \n",
    "            data_c = torch.load(osp.join(self.root, 'processed/run_{}_LF.pt'.format(i)))\n",
    "            data_m = torch.load(osp.join(self.root, 'processed/run_{}_MF.pt'.format(i)))\n",
    "            data_f = torch.load(osp.join(self.root, 'processed/run_{}_HF.pt'.format(i)))\n",
    "            \n",
    "        return [data_c, data_m, data_f] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb5a9eb",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b70e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the list of simulation numbers from the dataset\n",
    "### change this based on your data!!!\n",
    "mypath = 'dataset/'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "file_nums=[]\n",
    "for file in onlyfiles:\n",
    "    file_nums.append(int(file.split('_')[1]))\n",
    "file_nums = np.unique(np.array(file_nums)) \n",
    "\n",
    "# defining the root and source directory\n",
    "# root directory/processed is the folder where the processed graph data is stored\n",
    "# source directory contains the raw graph data\n",
    "root_dir = \"dataset/\"\n",
    "source_dir = \"dataset\"\n",
    "\n",
    "# 500 simulations are used for training and 300 for testing\n",
    "# you can changes these numbers\n",
    "train_list = file_nums[:500]\n",
    "val_list = file_nums[500:]\n",
    "\n",
    "#change batch size, if needed\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61f152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing training and validation datasets\n",
    "train_dataset = MFGraphDataset(root_dir, source_dir, train_list, test=False)\n",
    "val_dataset = MFGraphDataset(root_dir, source_dir, val_list, test=True)\n",
    "\n",
    "## Defining the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "## Calculating the min, max values of node, edge attributes and target for scaling purposes\n",
    "xmax_c,ymax_c,emax_c,xmin_c,ymin_c,emin_c,xmax_m,ymax_m,emax_m,xmin_m,ymin_m,emin_m,xmax_f,ymax_f,emax_f,xmin_f,ymin_f,emin_f = get_max_min(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f7e7f9",
   "metadata": {},
   "source": [
    "# GNN Model Architecture\n",
    "\n",
    "Next section contains the functions and classes which define the Multi-fidelity Graph U-Net architecture\n",
    "\n",
    "Classes EdgeModel and NodeModel contain the edge and node update MLP functions for message passing and propagation\n",
    "\n",
    "## Multi-fidelity Graph U-Net architecture (Net_MFGUNN_v1)\n",
    "\n",
    "This architecture is explained using three levels of graph resolution. This can be easily extrapolated to any number of resolutions.\n",
    "\n",
    "We consider three resolutions of graph, namely coarse, medium and fine with the graph data identified as data_c, data_m and data_f respectively. \n",
    "\n",
    "The node attributes of medium graph data contains the indices of the N nearest nodes (calculated using Euclidean distance during the data creation stage) of the corresponding fine graph, in addition to the node features used for training. Similarly, node attributes of coarse graph data contains the indices of the N nearest nodes of the corresponding medium graph as additional node attributes. Fine graph contains only the node features used for the training. These indices, which are the additional node attributes are extracted and stored in indices_c and indices_m from the coarse and medum graph respectively, during the training.\n",
    "\n",
    "We start with the fine graph, whose node and edge features are encoded to a latent space using an encoder function. \n",
    "The encoded node and edge attributes are passed through a number of GN blocks (composed of node and edge update modules) for message passing and propagation to update the node and edge attributes of the fine graph.\n",
    "\n",
    "Similar to fine graph, we encode the node and edge features of the medium graph. Then, the updated node attributes of the fine graph is added to the updated node attributes of the medium graph, through a nearest neighbor downsampling method. For this, we use the indices_m list which contains the N nearest nodes from the fine graph, for every node in the medium graph. For every node in the medium graph, we aggregate the node attributes of the N nearest nodes of the fine graph using a MEAN or MAX function and added to the encoded node attribute of the node with a learnable weight (weight_m). This updated node attributes and edge attributes are then passed through a number of GN blocks for message passing and aggregation. \n",
    "\n",
    "Similar process is done to transfer node attributes from medium graph to coarse graph, followed by a number of GN blocks for message passing and aggregation. \n",
    "\n",
    "This completes the downward flow of information of the U-Net architecture. This is followed by the upward flow direction, where nodal attribute information is upsampled and passed from coarse to medium to fine graph. For this, the same set of N nearest nodes are used for upsampling the information from one level to the other as can be seen in the function 'get_upsample_attr', which is added to the current node attributes of all the nodes in that level. \n",
    "\n",
    "Finally, the updated node attributes of coarse, medium and fine graphs are passed through the decoder function for target prediction at that level. For loss function, all the three target prediction outputs are used. \n",
    "\n",
    "The function 'get_upsample_attr' is used for getting the indices for upsampling values from the coarse graph to medium graph as well as from medium graph to fine graph\n",
    "\n",
    "## Multi-fidelity Graph U-Net architecture (Net_MFGUNN_v2)\n",
    "\n",
    "This is the version used for the results in the paper. Here, the GN blocks are shared across different resolution levels.\n",
    "\n",
    "First, encoded node attributes from high resolution graph is passed through k (defined by variable couple_point) GN blocks and then downsampled and the node attributes are added to the encoded node attributes of the medium resolution graph. The same process is done between medium and low resolution graphs. The node attributes of the low resolution graph is passed through all GN blocks and then they are upsampled to add to the medium resolution graph. this added node attributes of medium graph is passed through rest of the GN blocks. same process is done from medium to high resolution graph. \n",
    "\n",
    "Output from all levels are used for the loss function. \n",
    "\n",
    "## Multi-fidelity Graph U-Net Lite architecture (Net_MFGUNN_uni)\n",
    "\n",
    "This architecture is very similar to Net_MFUGNN_v2. The only difference is in the flow of information which is uni-directional here - from low to high resolution. So all the downsampling steps are avoided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b3a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeModel (torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Class for updating the edge attributes of the graph. \n",
    "    \n",
    "    It is a MLP network with a single hidden layer, with ReLU activation function\n",
    "    Input consists of edge attributes node attributes of the edges and the node attributes of the two nodes connected by the edge\n",
    "    \n",
    "    Output is the updated edge attribites of all the edges of the graph\n",
    "    \n",
    "    If residuals is True, the updated values are added to the previous edge attributes before they are returned.\n",
    "    This is similar to the residual network.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, n_edge_features, hiddens, n_targets, residuals):\n",
    "        super().__init__()\n",
    "        self.residuals = residuals\n",
    "        self. edge_mlp = Seq(\n",
    "            Linear(2*n_features + n_edge_features, hiddens),\n",
    "            ReLU(),\n",
    "            Linear (hiddens, n_targets),\n",
    "        )\n",
    "        \n",
    "    def forward(self, src, dest, edge_attr, u=None, batch=None):\n",
    "        #Concats the nodes connecting the edges and edge attributes and passed through MLP\n",
    "        #src and dest are the node attributes of the two nodes\n",
    "        #edge_attr is the edge attributes of the edge connecting the two nodes\n",
    "        \n",
    "        out = torch.cat([src, dest, edge_attr], 1)\n",
    "        out = self.edge_mlp(out)\n",
    "        if self.residuals:\n",
    "            out = out + edge_attr\n",
    "        return out\n",
    "\n",
    "class NodeModel (torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Class for updating the node attributes of the graph. \n",
    "    \n",
    "    It consists of two MLP networks. Both the networks have single hidden layer with with ReLU activation function.\n",
    "    \n",
    "    First MLP network, node_mlp_1, takes the node attributes of the neighboring nodes and the updated edge attributes of the\n",
    "    edges connecting these nodes as the input. Output is the message from the neighboring nodes, with the same size as the\n",
    "    node attributes. \n",
    "    \n",
    "    Second MLP network, node_mlp_2, takes the aggregated message acorss all the neighboring nodes from node_mlp_1 \n",
    "    and the node attributes of the current node as the input. Output is the updated node attributes. \n",
    "    \n",
    "    If residuals is True, the updated values are added to the previous node attributes before they are returned.\n",
    "    This is similar to the residual network.\n",
    "    \"\"\" \n",
    "    \n",
    "    def __init__(self, n_features, n_edge_features, hiddens, n_targets, residuals):\n",
    "        super(NodeModel, self).__init__()\n",
    "        \n",
    "        self.residuals = residuals\n",
    "        \n",
    "        #message calculation MLP\n",
    "        self. node_mlp_1 = Seq(\n",
    "            Linear(n_features + n_edge_features, hiddens),\n",
    "            ReLU(),\n",
    "            Linear(hiddens, n_targets),\n",
    "        )\n",
    "        \n",
    "        #node attribute update MLP\n",
    "        self.node_mlp_2 = Seq(\n",
    "            Linear (hiddens + n_features, hiddens),\n",
    "            ReLU(),\n",
    "            Linear(hiddens, n_targets),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        row, col = edge_index\n",
    "        out = torch. cat([x[col], edge_attr], dim=1)\n",
    "        out = self.node_mlp_1(out) #message calculation\n",
    "        out = torch_scatter.scatter_add(out, row, dim=0, dim_size=x.size(0)) #message aggregation, aggregation function is SUM\n",
    "        out = torch.cat([x, out], dim=1)\n",
    "        out = self.node_mlp_2(out) #node update\n",
    "        if self.residuals:\n",
    "            out = out + x\n",
    "        return out\n",
    "\n",
    "\n",
    "def build_layer(n_features, n_edge_features, hiddens, n_targets, batchnorm=False, residuals=True):\n",
    "    \"\"\"Calling the edge and node update modules\n",
    "    \"\"\" \n",
    "    return geom_nn.MetaLayer(\n",
    "        edge_model=EdgeModel(n_features, n_edge_features, hiddens, n_targets, residuals=residuals),\n",
    "        node_model=NodeModel(n_features, n_edge_features, hiddens, n_targets, residuals=residuals),\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9679b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_upsample_attr(indices, node_attr, N, channel, device):\n",
    "    \n",
    "    \"\"\"Loading the data from the source directory and returns the data\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    indices : a tensor containing the indices of all the nearest nodes of the next high level resolution of data, Tensor of shape k x N, Long\n",
    "    node_attr : a tensor with the updated node attributes for all the nodes for the current level resolution, Tensor of size k x channel, Float\n",
    "    N : number of nearest neighbors considered, int\n",
    "    channel : latent space size of the node attributes, int\n",
    "    device : CPU/GPU\n",
    "    \n",
    "    Ouput\n",
    "    ------\n",
    "    upsampled_attr : upsampled node attribute tensor to be added to the node attributes of all the nodes of the\n",
    "                    next high level resolution of data, Tensor of shape k x 1\n",
    "       \n",
    "    \"\"\" \n",
    "    #get the list of all the indices available in the tensor indices\n",
    "    #k x N tensor converted to kN x 1 tensor\n",
    "    indices = indices.reshape(-1,1).to(device)\n",
    "    \n",
    "    # uniq_index_list is the list of unique indices in the tensor, indices\n",
    "    # uniq_index_pos is the original position of the values in uniq_index_list in the tensor, indices\n",
    "    uniq_index_list, uniq_index_pos = torch.unique(indices, return_inverse=True)\n",
    "    \n",
    "    # creating a zero value tensor of with number of rows same as the number of unique indices\n",
    "    # and number of columns matching the channel value\n",
    "    zero_sum = torch.zeros(uniq_index_list.shape[0], channel, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # for every unique value in indices, counts gives the number of times its repeated in indices \n",
    "    _, counts = torch.unique(indices.to(device), return_counts=True)\n",
    "       \n",
    "    # repeats the node attributes N times    \n",
    "    node_attr_repeat = torch.repeat_interleave(node_attr, N, dim=0).to(device)\n",
    "    \n",
    "    # to the zero tensor, node attributes are added corresponding to the indices in uniq_index_pos\n",
    "    upsampled_attr = zero_sum.index_add_(0, uniq_index_pos[:,0], node_attr_repeat.to(device))\n",
    "    # when multiple node attributes are added a single index, the following step results in taking the mean of \n",
    "    # all the added node attributes\n",
    "    upsampled_attr/=counts[:, None]\n",
    "    \n",
    "    # all the upsampled node attributes are concatenated with the list of indices to which they belong\n",
    "    upsampled_attr = torch.hstack((uniq_index_list.reshape(-1,1).to(device), upsampled_attr.to(device)))\n",
    "    \n",
    "    return upsampled_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9560d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_MFGUNN_v1(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Class defining the Multi-Fidelity Graph U-Net architecture\n",
    "    \n",
    "    \"\"\" \n",
    "    def __init__(self, node_att_num, edge_att_num, output_dim, \n",
    "                 gn_depth_list, encoder_neurons, N, node_index_list,\n",
    "                 index_start, dropout=0.1, batch_norm = False):\n",
    "        super(Net_MFGUNN_v1, self).__init__()\n",
    "\n",
    "        self.node_att_num = node_att_num #number of node attributes\n",
    "        self.edge_att_num = edge_att_num #number of edge attributes\n",
    "        self.encoder_neurons = encoder_neurons #number of neurons in each layer of encoder network\n",
    "        self.output_dim = output_dim #number of outputs/targets\n",
    "        self.gn_depth_list = gn_depth_list #list containing the number of GN blocks for each level\n",
    "        self.node_index_list = node_index_list #indices of the node features to be used for training\n",
    "        self.dropout = dropout #droput ratio\n",
    "        self.N = N #number of nearest nodes considered\n",
    "        self.channel = self.encoder_neurons[1] #latent space size after encoding\n",
    "        self.index_start = index_start\n",
    "        # defining the encoder network for node attributes\n",
    "        self.encoder_node = Seq(Linear(self.node_att_num, self.encoder_neurons[0]),\n",
    "                                ReLU(),\n",
    "                                Linear(self.encoder_neurons[0], self.encoder_neurons[1]))\n",
    "        # defining the encoder network for edge attributes\n",
    "        self.encoder_edge = Seq(Linear(self.edge_att_num, self.encoder_neurons[0]),\n",
    "                                ReLU(),\n",
    "                                Linear(self.encoder_neurons[0], self.encoder_neurons[1]))\n",
    "        # defining the decoder network \n",
    "        self.decoder = Seq(Linear(self.encoder_neurons[1], self.encoder_neurons[0]),\n",
    "                                ReLU(),\n",
    "                                Linear(self.encoder_neurons[0], self.output_dim))\n",
    "        # defining batch norm if used\n",
    "        self.bn = torch.nn.BatchNorm1d(self.channel)\n",
    "        \n",
    "        # learnable weights with which the node attributes are added from fine to medium and medium to coarse graphs\n",
    "        self.weight_c = Linear(self.channel, self.channel)\n",
    "        self.weight_m = Linear(self.channel, self.channel)\n",
    "        \n",
    "        # defining the GN blocks for message passing and aggregation\n",
    "        self.GN_Blocks= torch.nn.ModuleList()\n",
    "        for k in range(len(self.gn_depth_list)):\n",
    "            self.GN_Blocks_k= torch.nn.ModuleList()\n",
    "            for i in range(self.gn_depth_list[k]):\n",
    "                self.GN_Blocks_k.append(build_layer(self.channel,self.channel,self.channel,self.channel))\n",
    "            self.GN_Blocks.append(self.GN_Blocks_k)\n",
    "        \n",
    "    def forward(self, data, device, scale=True):\n",
    "        \n",
    "        assert len(self.gn_depth_list) == 5, \"Invalid GN list length\" # GN list length should be 5 for 3 levels of resolution \n",
    "        \n",
    "        data_c = data[0].to(device) #coarse graph\n",
    "        data_m = data[1].to(device) #medium graph\n",
    "        data_f = data[2].to(device) #fine graph\n",
    "\n",
    "        x_c, edge_index_c, edge_attr_c = data_c.x, data_c.edge_index, data_c.edge_attr\n",
    "        x_m, edge_index_m, edge_attr_m = data_m.x, data_m.edge_index, data_m.edge_attr\n",
    "        x_f, edge_index_f, edge_attr_f = data_f.x, data_f.edge_index, data_f.edge_attr\n",
    "        \n",
    "        # extracting the indices stored as node attributes in medium and fine graph\n",
    "        indices_c = x_c[:,self.index_start:].to(device) \n",
    "        indices_m = x_m[:,self.index_start:].to(device)\n",
    "        \n",
    "        if scale:\n",
    "            # scaling node attributes\n",
    "            x_c = min_max_scaler(x_c, xmin_c, xmax_c, device)\n",
    "            x_m = min_max_scaler(x_m, xmin_m, xmax_m, device)\n",
    "            x_f = min_max_scaler(x_f, xmin_f, xmax_f, device)\n",
    "            # scaling edge attributes\n",
    "            edge_attr_c = min_max_scaler(edge_attr_c, emin_c, emax_c, device)\n",
    "            edge_attr_m = min_max_scaler(edge_attr_m, emin_m, emax_m, device)\n",
    "            edge_attr_f = min_max_scaler(edge_attr_f, emin_f, emax_f, device)\n",
    "\n",
    "        # encoding the node attributes for all three graphs\n",
    "        x_c = self.encoder_node(x_c[:,self.node_index_list])\n",
    "        x_m = self.encoder_node(x_m[:,self.node_index_list])\n",
    "        x_f = self.encoder_node(x_f[:,self.node_index_list])\n",
    "        \n",
    "        # encoding the edge attributes for all three graphs\n",
    "        edge_attr_c = self.encoder_edge(torch.tensor(edge_attr_c, dtype=torch.float))\n",
    "        edge_attr_m = self.encoder_edge(torch.tensor(edge_attr_m, dtype=torch.float))\n",
    "        edge_attr_f = self.encoder_edge(torch.tensor(edge_attr_f, dtype=torch.float))\n",
    "\n",
    "        # encoded attributes of fine graph are passed through the first set of GN blocks\n",
    "        for i in range(self.gn_depth_list[0]):\n",
    "            x_f, edge_attr_f, _ = self.GN_Blocks[0][i](x_f, edge_index_f, edge_attr=edge_attr_f)\n",
    "        \n",
    "        # for the nearest nodes from the fine graph, the node attributes are extracted and stored in x_m_downsample\n",
    "        x_m_downsample = x_f[indices_m.reshape(-1,1)[:,0].long(),:].reshape(-1, self.N, self.channel)\n",
    "        # the extracted node attributes are aggregated (i used mean, you can use max as well)\n",
    "        # they are then weighted using a learnable weight (weight_m) \n",
    "        # and added to the encoded node attributes of medium graph\n",
    "        x_m = x_m + self.weight_m(torch.mean(x_m_downsample,1)[0].to(device))\n",
    "          \n",
    "        # these node attributes of medium graph are then passed through the second set of GN blocks\n",
    "        for i in range(self.gn_depth_list[1]):\n",
    "            x_m, edge_attr2, _ = self.GN_Blocks[1][i](x_m, edge_index_m, edge_attr=edge_attr_m)\n",
    "\n",
    "        # for the nearest nodes from the medium graph, the node attributes are extracted and stored in x_c_downsample\n",
    "        x_c_downsample = x_m[indices_c.reshape(-1,1)[:,0].long(),:].reshape(-1, self.N, self.channel)\n",
    "        # which is then aggregated and added to the encoded node attributes of coarse graph\n",
    "        x_c = x_c + self.weight_c(torch.mean(x_c_downsample,1)[0].to(device))\n",
    "        \n",
    "        # these node attributes of coarse graph are then passed through the third set of GN blocks\n",
    "        for i in range(self.gn_depth_list[2]):\n",
    "            x_c, edge_attr_c, _ = self.GN_Blocks[2][i](x_c, edge_index_c, edge_attr=edge_attr_c)\n",
    "            \n",
    "            \n",
    "        # upsampling starts from here\n",
    "        # get the upsampling attributes from the coarse graph to be added to the medium graph using get_upsample_attr function\n",
    "        x_c_upsample = get_upsample_attr(indices_c.to(device), x_c.to(device), self.N, self.channel, device).to(device)\n",
    "        x_m[x_c_upsample[:,0].long(),:] = x_m[x_c_upsample[:,0].long(),:] + x_c_upsample[:,1:]\n",
    "        \n",
    "        # updated node attributes from medium graph passed through fourth set of GN blocks\n",
    "        for i in range(self.gn_depth_list[3]):\n",
    "            x_m, edge_attr_m, _ = self.GN_Blocks[3][i](x_m, edge_index_m, edge_attr=edge_attr_m)\n",
    "\n",
    "        # get the upsampling attributes from the medium graph to be added to the fine graph using get_upsample_attr function\n",
    "        x_m_upsample = get_upsample_attr(indices_m.to(device), x_m.to(device), self.N, self.channel, device).to(device)\n",
    "        x_f[x_m_upsample[:,0].long(),:] = x_f[x_m_upsample[:,0].long(),:] + x_m_upsample[:,1:]\n",
    "\n",
    "        # updated node attributes from fine graph passed through final set of GN blocks\n",
    "        for i in range(self.gn_depth_list[4]):\n",
    "            x_f, edge_attr_f, _ = self.GN_Blocks[4][i](x_f, edge_index_f, edge_attr=edge_attr_f)\n",
    "        \n",
    "        # all the updated node attributes from fine, coarse and medium graphs passed through decoder\n",
    "        # to get the predicted responses for each level of resolution\n",
    "        x_f = self.decoder(x_f)\n",
    "        x_m = self.decoder(x_m)\n",
    "        x_c = self.decoder(x_c)\n",
    "        \n",
    "        return x_c, x_m, x_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5285b2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_MFGUNN_v2(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Class defining the Multi-Fidelity Graph U-Net architecture\n",
    "    \n",
    "    \"\"\" \n",
    "    def __init__(self, node_att_num, edge_att_num, output_dim, \n",
    "                 gn_depth, encoder_neurons, N, node_index_list,\n",
    "                 index_start,couple_point, dropout=0.1, batch_norm = False):\n",
    "        super(Net_MFGUNN_v2, self).__init__()\n",
    "\n",
    "        self.node_att_num = node_att_num #number of node attributes\n",
    "        self.edge_att_num = edge_att_num #number of edge attributes\n",
    "        self.encoder_neurons = encoder_neurons #number of neurons in each layer of encoder network\n",
    "        self.output_dim = output_dim #number of outputs/targets\n",
    "        self.gn_depth = gn_depth #list containing the number of GN blocks for each level\n",
    "        self.node_index_list = node_index_list #indices of the node features to be used for training\n",
    "        self.dropout = dropout #droput ratio\n",
    "        self.N = N #number of nearest nodes considered\n",
    "        self.channel = self.encoder_neurons[1] #latent space size after encoding\n",
    "        self.index_start = index_start #index of the node attributes from where the list of indices of the k nearest neighbors from the next higher resolution is stored. This is calcualted during the data creation step offline.\n",
    "        # defining the encoder network for node attributes\n",
    "        self.encoder_node = Seq(Linear(self.node_att_num, self.encoder_neurons[0]),\n",
    "                                ReLU(),\n",
    "                                Linear(self.encoder_neurons[0], self.encoder_neurons[1]))\n",
    "        # defining the encoder network for edge attributes\n",
    "        self.encoder_edge = Seq(Linear(self.edge_att_num, self.encoder_neurons[0]),\n",
    "                                ReLU(),\n",
    "                                Linear(self.encoder_neurons[0], self.encoder_neurons[1]))\n",
    "        # defining the decoder network \n",
    "        self.decoder = Seq(Linear(self.encoder_neurons[1], self.encoder_neurons[0]),\n",
    "                                ReLU(),\n",
    "                                Linear(self.encoder_neurons[0], self.output_dim))\n",
    "        # defining batch norm if used\n",
    "        self.bn = torch.nn.BatchNorm1d(self.channel)\n",
    "        self.couple_point = couple_point\n",
    "        \n",
    "        # learnable weights with which the node attributes are added from fine to medium and medium to coarse graphs\n",
    "        self.weight_c = Linear(self.channel, self.channel)\n",
    "        self.weight_m = Linear(self.channel, self.channel)\n",
    "        \n",
    "        # defining the GN blocks for message passing and aggregation\n",
    "        self.GN_Blocks= torch.nn.ModuleList()\n",
    "        for i in range(self.gn_depth):\n",
    "            self.GN_Blocks.append(build_layer(self.channel,self.channel,self.channel,self.channel))\n",
    "        \n",
    "    def forward(self, data, device, scale=True):\n",
    "                \n",
    "        data_c = data[0].to(device) #coarse graph\n",
    "        data_m = data[1].to(device) #medium graph\n",
    "        data_f = data[2].to(device) #fine graph\n",
    "\n",
    "        x_c, edge_index_c, edge_attr_c = data_c.x, data_c.edge_index, data_c.edge_attr\n",
    "        x_m, edge_index_m, edge_attr_m = data_m.x, data_m.edge_index, data_m.edge_attr\n",
    "        x_f, edge_index_f, edge_attr_f = data_f.x, data_f.edge_index, data_f.edge_attr\n",
    "        \n",
    "        # extracting the indices stored as node attributes in medium and fine graph\n",
    "        indices_c = x_c[:,self.index_start:].to(device) \n",
    "        indices_m = x_m[:,self.index_start:].to(device)\n",
    "        \n",
    "        if scale:\n",
    "            # scaling node attributes\n",
    "            x_c = min_max_scaler(x_c, xmin_c, xmax_c, device)\n",
    "            x_m = min_max_scaler(x_m, xmin_m, xmax_m, device)\n",
    "            x_f = min_max_scaler(x_f, xmin_f, xmax_f, device)\n",
    "            # scaling edge attributes\n",
    "            edge_attr_c = min_max_scaler(edge_attr_c, emin_c, emax_c, device)\n",
    "            edge_attr_m = min_max_scaler(edge_attr_m, emin_m, emax_m, device)\n",
    "            edge_attr_f = min_max_scaler(edge_attr_f, emin_f, emax_f, device)\n",
    "\n",
    "        # encoding the node attributes for all three graphs\n",
    "        x_c = self.encoder_node(x_c[:,self.node_index_list])\n",
    "        x_m = self.encoder_node(x_m[:,self.node_index_list])\n",
    "        x_f = self.encoder_node(x_f[:,self.node_index_list])\n",
    "        \n",
    "        # encoding the edge attributes for all three graphs\n",
    "        edge_attr_c = self.encoder_edge(torch.tensor(edge_attr_c, dtype=torch.float))\n",
    "        edge_attr_m = self.encoder_edge(torch.tensor(edge_attr_m, dtype=torch.float))\n",
    "        edge_attr_f = self.encoder_edge(torch.tensor(edge_attr_f, dtype=torch.float))\n",
    "\n",
    "        # encoded attributes of fine graph are passed through the couple_point set of GN blocks\n",
    "        for i in range(self.gn_depth-self.couple_point):\n",
    "            x_f, edge_attr_f, _ = self.GN_Blocks[i](x_f, edge_index_f, edge_attr=edge_attr_f)\n",
    "        \n",
    "        # for the nearest nodes from the fine graph, the node attributes are extracted and stored in x_m_downsample\n",
    "        x_m_downsample = x_f[indices_m.reshape(-1,1)[:,0].long(),:].reshape(-1, self.N, self.channel)\n",
    "        # the extracted node attributes are aggregated (i used mean, you can use max as well)\n",
    "        # they are then weighted using a learnable weight (weight_m) \n",
    "        # and added to the encoded node attributes of medium graph\n",
    "        x_m = x_m + self.weight_m(torch.mean(x_m_downsample,1)[0].to(device))\n",
    "          \n",
    "        # these node attributes of medium graph are then passed through the couple_point set of GN blocks\n",
    "        for i in range(self.gn_depth-self.couple_point):\n",
    "            x_m, edge_attr2, _ = self.GN_Blocks[i](x_m, edge_index_m, edge_attr=edge_attr_m)\n",
    "\n",
    "        # for the nearest nodes from the medium graph, the node attributes are extracted and stored in x_c_downsample\n",
    "        x_c_downsample = x_m[indices_c.reshape(-1,1)[:,0].long(),:].reshape(-1, self.N, self.channel)\n",
    "        # which is then aggregated and added to the encoded node attributes of coarse graph\n",
    "        x_c = x_c + self.weight_c(torch.mean(x_c_downsample,1)[0].to(device))\n",
    "        \n",
    "        # these node attributes of coarse graph are then passed through the entire set of GN blocks\n",
    "        for i in range(self.gn_depth_list):\n",
    "            x_c, edge_attr_c, _ = self.GN_Blocks[i](x_c, edge_index_c, edge_attr=edge_attr_c)\n",
    "            \n",
    "            \n",
    "        # upsampling starts from here\n",
    "        # get the upsampling attributes from the coarse graph to be added to the medium graph using get_upsample_attr function\n",
    "        x_c_upsample = get_upsample_attr(indices_c.to(device), x_c.to(device), self.N, self.channel, device).to(device)\n",
    "        x_m[x_c_upsample[:,0].long(),:] = x_m[x_c_upsample[:,0].long(),:] + x_c_upsample[:,1:]\n",
    "        \n",
    "        # updated node attributes from medium graph passed through fourth set of GN blocks\n",
    "        for i in range(self.gn_depth-self.couple_point,self.gn_depth_list):\n",
    "            x_m, edge_attr_m, _ = self.GN_Blocks[i](x_m, edge_index_m, edge_attr=edge_attr_m)\n",
    "\n",
    "        # get the upsampling attributes from the medium graph to be added to the fine graph using get_upsample_attr function\n",
    "        x_m_upsample = get_upsample_attr(indices_m.to(device), x_m.to(device), self.N, self.channel, device).to(device)\n",
    "        x_f[x_m_upsample[:,0].long(),:] = x_f[x_m_upsample[:,0].long(),:] + x_m_upsample[:,1:]\n",
    "\n",
    "        # updated node attributes from fine graph passed through final set of GN blocks\n",
    "        for i in range(self.gn_depth-self.couple_point, self.gn_depth_list):\n",
    "            x_f, edge_attr_f, _ = self.GN_Blocks[i](x_f, edge_index_f, edge_attr=edge_attr_f)\n",
    "        \n",
    "        # all the updated node attributes from fine, coarse and medium graphs passed through decoder\n",
    "        # to get the predicted responses for each level of resolution\n",
    "        x_f = self.decoder(x_f)\n",
    "        x_m = self.decoder(x_m)\n",
    "        x_c = self.decoder(x_c)\n",
    "        \n",
    "        return x_c, x_m, x_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e91e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_MFGUNN_uni(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Class defining the Multi-Fidelity Graph U-Net architecture\n",
    "    \n",
    "    \"\"\" \n",
    "    def __init__(self, node_att_num, edge_att_num, output_dim, \n",
    "                 gn_depth, encoder_neurons, N, node_index_list,\n",
    "                 index_start,couple_point, dropout=0.1, batch_norm = False):\n",
    "        super(Net_MFGUNN_uni, self).__init__()\n",
    "\n",
    "        self.node_att_num = node_att_num #number of node attributes\n",
    "        self.edge_att_num = edge_att_num #number of edge attributes\n",
    "        self.encoder_neurons = encoder_neurons #number of neurons in each layer of encoder network\n",
    "        self.output_dim = output_dim #number of outputs/targets\n",
    "        self.gn_depth = gn_depth #list containing the number of GN blocks for each level\n",
    "        self.node_index_list = node_index_list #indices of the node features to be used for training\n",
    "        self.dropout = dropout #droput ratio\n",
    "        self.N = N #number of nearest nodes considered\n",
    "        self.channel = self.encoder_neurons[1] #latent space size after encoding\n",
    "        self.index_start = index_start\n",
    "        # defining the encoder network for node attributes\n",
    "        self.encoder_node = Seq(Linear(self.node_att_num, self.encoder_neurons[0]),\n",
    "                                ReLU(),\n",
    "                                Linear(self.encoder_neurons[0], self.encoder_neurons[1]))\n",
    "        # defining the encoder network for edge attributes\n",
    "        self.encoder_edge = Seq(Linear(self.edge_att_num, self.encoder_neurons[0]),\n",
    "                                ReLU(),\n",
    "                                Linear(self.encoder_neurons[0], self.encoder_neurons[1]))\n",
    "        # defining the decoder network \n",
    "        self.decoder = Seq(Linear(self.encoder_neurons[1], self.encoder_neurons[0]),\n",
    "                                ReLU(),\n",
    "                                Linear(self.encoder_neurons[0], self.output_dim))\n",
    "        # defining batch norm if used\n",
    "        self.bn = torch.nn.BatchNorm1d(self.channel)\n",
    "        self.couple_point = couple_point\n",
    "        \n",
    "        # learnable weights with which the node attributes are added from fine to medium and medium to coarse graphs\n",
    "        self.weight_c = Linear(self.channel, self.channel)\n",
    "        self.weight_m = Linear(self.channel, self.channel)\n",
    "        \n",
    "        # defining the GN blocks for message passing and aggregation\n",
    "        self.GN_Blocks= torch.nn.ModuleList()\n",
    "        for i in range(self.gn_depth):\n",
    "            self.GN_Blocks.append(build_layer(self.channel,self.channel,self.channel,self.channel))\n",
    "        \n",
    "    def forward(self, data, device, scale=True):\n",
    "                \n",
    "        data_c = data[0].to(device) #coarse graph\n",
    "        data_m = data[1].to(device) #medium graph\n",
    "        data_f = data[2].to(device) #fine graph\n",
    "\n",
    "        x_c, edge_index_c, edge_attr_c = data_c.x, data_c.edge_index, data_c.edge_attr\n",
    "        x_m, edge_index_m, edge_attr_m = data_m.x, data_m.edge_index, data_m.edge_attr\n",
    "        x_f, edge_index_f, edge_attr_f = data_f.x, data_f.edge_index, data_f.edge_attr\n",
    "        \n",
    "        # extracting the indices stored as node attributes in medium and fine graph\n",
    "        indices_c = x_c[:,self.index_start:].to(device) \n",
    "        indices_m = x_m[:,self.index_start:].to(device)\n",
    "        \n",
    "        if scale:\n",
    "            # scaling node attributes\n",
    "            x_c = min_max_scaler(x_c, xmin_c, xmax_c, device)\n",
    "            x_m = min_max_scaler(x_m, xmin_m, xmax_m, device)\n",
    "            x_f = min_max_scaler(x_f, xmin_f, xmax_f, device)\n",
    "            # scaling edge attributes\n",
    "            edge_attr_c = min_max_scaler(edge_attr_c, emin_c, emax_c, device)\n",
    "            edge_attr_m = min_max_scaler(edge_attr_m, emin_m, emax_m, device)\n",
    "            edge_attr_f = min_max_scaler(edge_attr_f, emin_f, emax_f, device)\n",
    "\n",
    "        # encoding the node attributes for all three graphs\n",
    "        x_c = self.encoder_node(x_c[:,self.node_index_list])\n",
    "        x_m = self.encoder_node(x_m[:,self.node_index_list])\n",
    "        x_f = self.encoder_node(x_f[:,self.node_index_list])\n",
    "        \n",
    "        # encoding the edge attributes for all three graphs\n",
    "        edge_attr_c = self.encoder_edge(torch.tensor(edge_attr_c, dtype=torch.float))\n",
    "        edge_attr_m = self.encoder_edge(torch.tensor(edge_attr_m, dtype=torch.float))\n",
    "        edge_attr_f = self.encoder_edge(torch.tensor(edge_attr_f, dtype=torch.float))\n",
    "        \n",
    "        #  node attributes of fine graph are  passed through upto couple_point GN blocks\n",
    "        for i in range(self.gn_depth-self.couple_point):\n",
    "            x_f, edge_attr_f, _ = self.GN_Blocks[i](x_f, edge_index_f, edge_attr=edge_attr_f)\n",
    "        #  node attributes of coarse graph are  passed through upto couple_point GN blocks\n",
    "        for i in range(self.gn_depth-self.couple_point):\n",
    "            x_m, edge_attr2, _ = self.GN_Blocks[i](x_m, edge_index_m, edge_attr=edge_attr_m)\n",
    "        \n",
    "        #  node attributes of coarse graph are  passed through the entire set of GN blocks\n",
    "        for i in range(self.gn_depth_list):\n",
    "            x_c, edge_attr_c, _ = self.GN_Blocks[i](x_c, edge_index_c, edge_attr=edge_attr_c)\n",
    "            \n",
    "        # upsampling starts from here\n",
    "        # get the upsampling attributes from the coarse graph to be added to the medium graph using get_upsample_attr function\n",
    "        x_c_upsample = get_upsample_attr(indices_c.to(device), x_c.to(device), self.N, self.channel, device).to(device)\n",
    "        x_m[x_c_upsample[:,0].long(),:] = x_m[x_c_upsample[:,0].long(),:] + x_c_upsample[:,1:]\n",
    "        \n",
    "        # updated node attributes from medium graph passed through fourth set of GN blocks\n",
    "        for i in range(self.gn_depth-self.couple_point,self.gn_depth_list):\n",
    "            x_m, edge_attr_m, _ = self.GN_Blocks[i](x_m, edge_index_m, edge_attr=edge_attr_m)\n",
    "\n",
    "        # get the upsampling attributes from the medium graph to be added to the fine graph using get_upsample_attr function\n",
    "        x_m_upsample = get_upsample_attr(indices_m.to(device), x_m.to(device), self.N, self.channel, device).to(device)\n",
    "        x_f[x_m_upsample[:,0].long(),:] = x_f[x_m_upsample[:,0].long(),:] + x_m_upsample[:,1:]\n",
    "\n",
    "        # updated node attributes from fine graph passed through final set of GN blocks\n",
    "        for i in range(self.gn_depth-self.couple_point, self.gn_depth_list):\n",
    "            x_f, edge_attr_f, _ = self.GN_Blocks[i](x_f, edge_index_f, edge_attr=edge_attr_f)\n",
    "        \n",
    "        # all the updated node attributes from fine, coarse and medium graphs passed through decoder\n",
    "        # to get the predicted responses for each level of resolution\n",
    "        x_f = self.decoder(x_f)\n",
    "        x_m = self.decoder(x_m)\n",
    "        x_c = self.decoder(x_c)\n",
    "        \n",
    "        return x_c, x_m, x_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4b825a",
   "metadata": {},
   "source": [
    "# Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99c690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(data_loader, loss_all, device, scale):\n",
    "    \"\"\"Training the GNN model\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data_loader : Data loader object from pytorch geometric, it contains all the graphs for training\n",
    "    loss_all : loss value, Tensor float\n",
    "    device : GPU/CPU\n",
    "    scale : if True, scaling is done on node and edge attributes as well as the target, boolean\n",
    "    \n",
    "    Ouput\n",
    "    ------\n",
    "    loss_all : loss value after a single epoch, Tensor float\n",
    "       \n",
    "    \"\"\" \n",
    "    model.train()\n",
    "    for data in data_loader:\n",
    "        # get the predicted outputs\n",
    "        out_c, out_m, out_f = model(data, device, scale)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # scale target responses if scale is True\n",
    "        if scale:\n",
    "            y_c =  min_max_scaler(data[0].y, ymin_c, ymax_c, device).reshape(-1,1)\n",
    "            y_m =  min_max_scaler(data[1].y, ymin_m, ymax_m, device).reshape(-1,1)\n",
    "            y_f =  min_max_scaler(data[2].y, ymin_f, ymax_f, device).reshape(-1,1)\n",
    "        else:\n",
    "            y_c = data[0].y.reshape(-1,1)\n",
    "            y_m = data[1].y.reshape(-1,1)\n",
    "            y_f = data[2].y.reshape(-1,1)\n",
    "            \n",
    "        # loss calculation\n",
    "        # loss is calculated for predictions from coarse, medium and fine graphs\n",
    "        # they are weighted in tha ratio 1:5:10 for coarse:medium:fine\n",
    "        loss_calc = loss(out_c.reshape(-1,1), y_c.reshape(-1,1))  + 5*loss(out_m.reshape(-1,1), y_m.reshape(-1,1)) + 10*loss(out_f.reshape(-1,1), y_f.reshape(-1,1))\n",
    "        loss_all += data[2].num_graphs * loss_calc.item()\n",
    "        loss_calc.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        my_lr_scheduler.step()\n",
    "        \n",
    "    return loss_all\n",
    "\n",
    "\n",
    "def model_eval(data_loader, device, scale):\n",
    "    \"\"\"Evaluating the GNN model\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data_loader : Data loader object from pytorch geometric, it contains all the graphs for training\n",
    "    device : GPU/CPU\n",
    "    scale : if True, scaling is done on node and edge attributes as well as the target, boolean\n",
    "    \n",
    "    Ouput\n",
    "    ------\n",
    "    l2_err : relative L2 error for the predictions in the fine graph, float\n",
    "       \n",
    "    \"\"\" \n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    for data in data_loader:\n",
    "                \n",
    "        #getting the prediction from just the fine graph\n",
    "        _, _, pred = model(data, device, scale)\n",
    "        \n",
    "        if scale:\n",
    "            pred = min_max_descaler(pred, ymin_f, ymax_f, device).detach().cpu().numpy().reshape(-1,1)\n",
    "        else:\n",
    "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
    "            \n",
    "        label = data[2].y.detach().cpu().numpy().reshape(-1,1)\n",
    "        predictions.append(pred)\n",
    "        labels.append(label)\n",
    "        \n",
    "    predictions = np.vstack(predictions)\n",
    "    labels = np.vstack(labels)\n",
    "    \n",
    "    # calculation of relative L2 error\n",
    "    diff_norm = np.linalg.norm(predictions - labels, ord=2)\n",
    "    y_norm = np.linalg.norm(labels, ord=2)\n",
    "    l2_err = np.mean(diff_norm / y_norm)\n",
    "\n",
    "    return l2_err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dec07f",
   "metadata": {},
   "source": [
    "### CHANGE TRAINING AND MODEL PARAMETERS HERE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f79605",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TRAINING HYPERPARAMETERS\n",
    "n_epochs = 1000\n",
    "batch_size = 2\n",
    "lr = 0.001\n",
    "weight_decay=1e-6\n",
    "\n",
    "# MODEL PARAMETERS\n",
    "# change these based on your data\n",
    "node_att_num = 10\n",
    "edge_att_num = 3\n",
    "output_dim = 1\n",
    "gn_depth_list = [3,2,2,2,3]\n",
    "gn_depth = 12\n",
    "couple_point = 6\n",
    "index_start = 12\n",
    "encoder_neurons = [64, 128]\n",
    "node_index_list = [0,1,4,5,6,7,8,9,10,11]\n",
    "N = 5\n",
    "dropout = 0.0\n",
    "scale = True\n",
    "batch_norm = False\n",
    "\n",
    "# DIRECTORIES TO STORE RESULTS\n",
    "result_dir = 'results'\n",
    "model_dir = 'models'\n",
    "loss_dir = 'losses'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a0c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#CHOOSE THE MODEL YOU WANT!!\n",
    "\n",
    "# model = Net_MFGUNN_v1(node_att_num=node_att_num, edge_att_num=edge_att_num, output_dim=output_dim, N=5,\n",
    "#                 gn_depth_list=gn_depth_list,node_index_list=node_index_list,encoder_neurons=encoder_neurons, \n",
    "#                 index_start=index_start,dropout=dropout, batch_norm = False).to(device)\n",
    "\n",
    "model = Net_MFGUNN_v2(node_att_num=node_att_num, edge_att_num=edge_att_num, output_dim=output_dim, N=5,\n",
    "                gn_depth=gn_depth,couple_point = couple_point, node_index_list=node_index_list,encoder_neurons=encoder_neurons, \n",
    "                index_start=index_start,dropout=dropout, batch_norm = False).to(device)\n",
    "\n",
    "# model = Net_MFGUNN_uni(node_att_num=node_att_num, edge_att_num=edge_att_num, output_dim=output_dim, N=5,\n",
    "#                 gn_depth=gn_depth,couple_point = couple_point, node_index_list=node_index_list,encoder_neurons=encoder_neurons, \n",
    "#                 index_start=index_start,dropout=dropout, batch_norm = False).to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.99, 0.999), weight_decay=weight_decay)\n",
    "my_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=n_epochs, eta_min=1e-8)\n",
    "\n",
    "# we can use MSE or relative L2 error loss for training\n",
    "\n",
    "#loss = torch.nn.MSELoss()\n",
    "def loss(pred, actual):\n",
    "    diff_norm = torch.norm(torch.flatten(pred)-torch.flatten(actual), p=2)\n",
    "    actual_norm = torch.norm(torch.flatten(actual), p=2)\n",
    "    return diff_norm/actual_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8c092",
   "metadata": {},
   "source": [
    "### MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model training\n",
    "epoch_list = []\n",
    "train_l2_err = []\n",
    "val_l2_err = []\n",
    "\n",
    "print('Training started...')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss_all = 0\n",
    "    loss_all = model_train(train_loader, loss_all, device, scale)\n",
    "    if(epoch%10==0):\n",
    "        epoch_list.append(epoch)\n",
    "        l2_err = model_eval(train_loader, device, scale)\n",
    "        train_l2_err.append(l2_err)\n",
    "        l2_err = model_eval(val_loader, device, scale)\n",
    "        val_l2_err.append(l2_err)\n",
    "        print('epoch: ', epoch, 'train error: ', train_l2_err[-1], 'val error: ', val_l2_err[-1])\n",
    "        print()\n",
    "\n",
    "        # saving the model\n",
    "        torch.save({\n",
    "            'epoch':epoch,\n",
    "            'model_state_dict':model.state_dict(),\n",
    "            'optimizer_state_dict':optimizer.state_dict(),\n",
    "        }, result_dir + '/' + model_dir + '/model_mfgunet.pt')\n",
    "        \n",
    "        # saving the loss results\n",
    "        np.savetxt(result_dir + '/' + loss_dir + '/train_l2_err.txt', train_l2_err)\n",
    "        np.savetxt(result_dir + '/' + loss_dir + '/val_l2_err.txt', val_l2_err)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41242e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTION FOR GETTING THE K NEAREST NODES DURING THE DATA GENERATION PART!!\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def get_topK_nodes(coarse_data, fine_data, K):\n",
    "    dist12 = euclidean_distances(coarse_data, fine_data)\n",
    "    top12 = np.argsort(dist12)[::-1][:,:K]\n",
    "    return top12"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
